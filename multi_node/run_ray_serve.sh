serve run llm:build_app port=8089 model="/mnt/hf_models/Qwen-14B-Chat" tensor-parallel-size=4 served-model-name="qwen1.5-7b"
